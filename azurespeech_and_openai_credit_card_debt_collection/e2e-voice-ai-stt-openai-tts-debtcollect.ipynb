{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "############# Update the following with your Azure OpenAI and Speech keys #############\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key=\"<azure_openai_key>\",\n",
    "  azure_endpoint=\"<azure_openai_endpoint>\",\n",
    "  api_version = \"2024-02-15-preview\"\n",
    "  \n",
    ")\n",
    "\n",
    "AOAI_model  = \"gpt-35-turbo-0125\" # \"gpt-35-turbo-16k\"  #\"gpt4\"\n",
    "\n",
    "speech_key = os.getenv(\"SPEECH_API_KEY\", \"<Azure_speech_key>\")\n",
    "service_region = \"<speech_region>\" #southeastasia\n",
    "\n",
    "\n",
    "languages = {\n",
    "\n",
    "    # \"stt\": \"zh-CN\",\n",
    "    # \"tts\": \"zh-CN-XiaoxiaoMultilingualNeural\", #\"zh-CN-XiaoyanNeural\", \n",
    "\n",
    "    \"stt\": \"en-SG\",\n",
    "    \"tts\": \"en-US-AvaMultilingualNeural\", #en-SG-LunaNeural\",\n",
    "\n",
    "    # \"stt\": \"ms-MY\",\n",
    "    # \"tts\": \"ms-MY-YasminNeural\",\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "user_profile = {\n",
    "    \"Name\": \"Mandy\",\n",
    "    \"Outstanding Debt Product\": \"Credit Card\",\n",
    "    \"Outstanding Debt Amount\": \"USD 500\",\n",
    "    \"Date to make payment\": \"2024-04-18\",\n",
    "    \"Minimum Payment\": \"USD 50\",\n",
    "    \"Preferred Language\": f\"{languages['stt']}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"As a voice assistant for a RichRichMoney bank's debt collection department, your goals are to inform customers about their outstanding debts and inquire about their payment plans. Follow the conversation Flow and Guidelines provided.\n",
    "\n",
    "Flow:\n",
    "- Announce that the conversation is AI-generated.\n",
    "- Greet the customer on behalf of the bank and confirm their identity.\n",
    "- Once confirmed, inform them about their outstanding debt on a specific product that is due on given date. Do not disclose the amount unless customer asked. Never offer customer to check on the due amount.\n",
    "- Inquire about the payment date and amount, asking if they plan to make a minimum or full payment.\n",
    "- If they agree to pay, thank them and conclude the conversation.\n",
    "- If they refuse, ask why and inform them a human agent will contact them.\n",
    "- If objectives are met, thank the customer and end the conversation.\n",
    "- If the customer is uncooperative, inform them a human agent will contact them and end the conversation.\n",
    "- Use \"Bye\" to end the conversation.\n",
    "\n",
    "Guidelines:\n",
    "- Maintain a polite, professional tone.\n",
    "- Use simple, short and concise language.\n",
    "- Respond in the customer's preferred language (English, Malay, or Chinese Simplified).\n",
    "- Keep the conversation focused on debt collection.\n",
    "- Do not disclose personal information.\n",
    "- You can disclose which bank you are representing.\n",
    "- Understand the customer's intent from their transcribed input. If unclear, repeat and confirm.\n",
    "- Do not ask/offer any help or assistant needed.\n",
    "\n",
    "\n",
    "Sensitive Customer Information:\n",
    "Name: {user_profile[\"Name\"]}\n",
    "Outstanding Debt Product: {user_profile[\"Outstanding Debt Product\"]}\n",
    "Outstanding Debt Amount: {user_profile[\"Outstanding Debt Amount\"]}\n",
    "Date to make payment: {user_profile[\"Date to make payment\"]}\n",
    "Minimum Payment: {user_profile[\"Minimum Payment\"]}\n",
    "Preferred Language: {user_profile[\"Preferred Language\"]}\n",
    "\n",
    "\n",
    "Current Context:\n",
    "Current Date: {datetime.now().strftime('%Y-%m-%d, %A')}\n",
    "Location: Kuala Lumpur, Malaysia\n",
    "\n",
    "\n",
    "Self check whether is following the Flow and Guidelines strictly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if languages[\"stt\"] == \"zh-CN\":\n",
    "    assistant_first_prompt=f\"您好，我是RichRichMoney银行的人工智能语音助手。 我正在是和 {user_profile['Name']} 说话吗?\"\n",
    "elif languages[\"stt\"] == \"ms-MY\":\n",
    "    assistant_first_prompt=f\"Hai, saya ialah pembantu suara yang dijana AI daripada bank RichRichMoney. Adakah saya bercakap dengan {user_profile['Name']} ?\"\n",
    "else:\n",
    "    languages[\"stt\"] == \"en-SG\"\n",
    "    assistant_first_prompt=f\"Hi, I am a AI generated voice assistant from a RichRichMoney bank. Am I speaking to {user_profile['Name']} ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text To Speech  - Continoues (3s) or detect a stop voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time \n",
    "\n",
    "PAUSE_SECONDS = 3\n",
    "\n",
    "def speech_recognize_continuous_from_mic():\n",
    "    \"\"\"performs continuous speech recognition with input from an audio file\"\"\"\n",
    "\n",
    "    # speech_key = os.getenv(\"SPEECH_API_KEY\", \"\")\n",
    "    # service_region = \"southeastasia\"\n",
    "\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    speech_config.speech_recognition_language = languages[\"stt\"] #\"en-SG\" #\"ms-MY\" #\"zh-CN\" #\"en-US\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    done = False\n",
    "    capture_sentence = \"\"\n",
    "    recognized_time  = time.time()\n",
    "    start_detect_silent = False\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "    \n",
    "    def recognized (evt):\n",
    "        print('RECOGNIZED: {}'.format(evt))\n",
    "        if 'ai' in evt.result.text.lower():\n",
    "            print('AI detected, stopping')\n",
    "            stop_cb(evt)\n",
    "            speech_recognizer.stop_continuous_recognition()\n",
    "            return\n",
    "        nonlocal capture_sentence, recognized_time, start_detect_silent\n",
    "        capture_sentence += evt.result.text\n",
    "        print(capture_sentence)\n",
    "        start_detect_silent = True\n",
    "        recognized_time = time.time()\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt.result.text))) #lambda evt: print('RECOGNIZING: {}'.format(evt))\n",
    "    speech_recognizer.recognized.connect(recognized)\n",
    "    speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "    # stop continuous recognition on either session stopped or canceled events\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "        pause_time = time.time()\n",
    "        if start_detect_silent == True and pause_time - recognized_time > PAUSE_SECONDS:\n",
    "            print(f\"No sound detected for {PAUSE_SECONDS} seconds, stopping\")\n",
    "            stop_cb(\"\")\n",
    "            speech_recognizer.stop_continuous_recognition()\n",
    "            break\n",
    "    print(\"#\" * 8)\n",
    "    print (capture_sentence)\n",
    "    print(\"#\" * 8)\n",
    "    return capture_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI - Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "memory = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"assistant\" , \"content\": assistant_first_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Define the Azure OpenAI language generation function\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    if prompt == None: return\n",
    "\n",
    "    memory.append( \n",
    "        {\"role\": \"user\", \"content\": prompt} \n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model = AOAI_model,\n",
    "        messages = memory,\n",
    "        temperature=0,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    memory.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    print (f\"*\" * 8)\n",
    "    print (f\"generated answer: {answer}\")\n",
    "    print (f\"*\" * 8)\n",
    "    return answer\n",
    "    # return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech to Text - Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Define the text-to-speech function\n",
    "def text_to_speech(text):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    speech_config.speech_synthesis_language = languages[\"tts\"][:6]\n",
    "\n",
    "    # Set up the voice configuration\n",
    "    speech_config.speech_synthesis_voice_name = languages[\"tts\"] #\"en-NZ-MollyNeural\"\n",
    "    #https://learn.microsoft.com/en-us/javascript/api/microsoft-cognitiveservices-speech-sdk/speechsynthesisoutputformat?view=azure-node-latest\n",
    "    # Audio16Khz32KBitRateMonoMp3\n",
    "    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Riff16Khz16BitMonoPcm) #Audio16Khz32KBitRateMonoMp3 #Riff16Khz16BitMonoPcm\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "    try:\n",
    "        result = speech_synthesizer.speak_text_async(text).get()\n",
    "        print(result)\n",
    "        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "            print(\"Text-to-speech conversion successful.\")\n",
    "            audio_data = result.audio_data\n",
    "            # change the file extension to match the format in line near 10\n",
    "            with open(\"output_audio.wav\", \"wb\") as audio_file:\n",
    "                audio_file.write(audio_data)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error synthesizing audio: {result}\")\n",
    "            return False\n",
    "    except Exception as ex:\n",
    "        print(f\"Error synthesizing audio: {ex}\")\n",
    "        return False\n",
    "# text_to_speech(\"how are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI voice (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def generate_speech_with_openai_tts(tts_text):\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts\", #check the azure portal for the deployment name\n",
    "        voice= \"nova\", #\"alloy\",\n",
    "        input=tts_text #text_answer\n",
    "    )\n",
    "\n",
    "\n",
    "    response.stream_to_file(\"output1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "def play_mp3(file_path):\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(file_path)\n",
    "    pygame.mixer.music.play()\n",
    "    \n",
    "    #release the audio file after playing\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "    pygame.mixer.quit()\n",
    "\n",
    "# Replace 'your_file.mp3' with your actual file path\n",
    "# play_mp3('output1.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "memory = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information. Answer it short and concise.\"},\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"assistant\" , \"content\": assistant_first_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Press space bar to start for every turn, run the cell above to delete the memory (reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import keyboard\n",
    "import time\n",
    "\n",
    "first_time = True\n",
    "\n",
    "#################### Microsoft TTS or OpenAI TTS ####################\n",
    "text_to_speech_option = \"openai\" # \"msft\" or \"openai\"\n",
    "#################### Microsoft TTS or OpenAI TTS ####################\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if keyboard.is_pressed('space'):  # if spacebar is pressed \n",
    "            if first_time:\n",
    "                if text_to_speech_option == \"msft\":\n",
    "                    text_to_speech(assistant_first_prompt)\n",
    "                elif text_to_speech_option == \"openai\":\n",
    "                    generate_speech_with_openai_tts(assistant_first_prompt)\n",
    "                    play_mp3(\"output1.mp3\")\n",
    "                    \n",
    "                first_time = False\n",
    "                continue\n",
    "        \n",
    "            speaker_text = speech_recognize_continuous_from_mic()\n",
    "            # time to generate text and speech\n",
    "\n",
    "            gen_start_time = time.time()\n",
    "            answer = generate_answer(speaker_text)\n",
    "            gen_end_time = time.time()\n",
    "            print(f\"time openai gen:{gen_end_time - gen_start_time  }\")\n",
    "\n",
    "            if text_to_speech_option == \"msft\":\n",
    "                ### Microsoft AI TTS\n",
    "                tts_start_time = time.time()\n",
    "                text_to_speech(answer) # Microsoft AI TTS\n",
    "                tts_stop_time = time.time()\n",
    "                print(f\"time tts gen:{tts_stop_time - tts_start_time }\")\n",
    "            elif text_to_speech_option == \"openai\":\n",
    "                \n",
    "                ### OpenAI TTS\n",
    "                tts_start_time = time.time()\n",
    "                generate_speech_with_openai_tts(answer)# Microsoft AI TTSl\n",
    "                tts_stop_time = time.time()\n",
    "                play_mp3(\"output1.mp3\")\n",
    "                print(f\"time tts gen:{tts_stop_time - tts_start_time }\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            diff_time = tts_stop_time - gen_start_time \n",
    "            \n",
    "            # print(memory)\n",
    "            print(f'time taken for text gen and speech gen:{diff_time} seconds')\n",
    "\n",
    "            if '再见' in answer.lower() or 'bye' in answer.lower():\n",
    "                break\n",
    "        elif keyboard.is_pressed('esc'):\n",
    "            break\n",
    "except KeyboardInterrupt: \n",
    "    print(\"Stopped by User\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
